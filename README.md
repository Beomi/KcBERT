# KcBERT: Korean comments BERT

** Updates on 2021.03.14 **

- KcBERT Paper ì¸ìš© í‘œê¸°ë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.(bibtex)
- KcBERT-finetune Performance scoreë¥¼ ë³¸ë¬¸ì— ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.

** Updates on 2020.12.04 **

Huggingface Transformersê°€ v4.0.0ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨ì— ë”°ë¼ Tutorialì˜ ì½”ë“œê°€ ì¼ë¶€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.

ì—…ë°ì´íŠ¸ëœ KcBERT-Large NSMC Finetuning Colab: <a href="https://colab.research.google.com/drive/1dFC0FL-521m7CL_PSd8RLKq67jgTJVhL?usp=sharing">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

** Updates on 2020.09.11 **

KcBERTë¥¼ Google Colabì—ì„œ TPUë¥¼ í†µí•´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” íŠœí† ë¦¬ì–¼ì„ ì œê³µí•©ë‹ˆë‹¤! ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ë³´ì„¸ìš”.

Colabì—ì„œ TPUë¡œ KcBERT Pretrain í•´ë³´ê¸°: <a href="https://colab.research.google.com/drive/1lYBYtaXqt9S733OXdXvrvC09ysKFN30W">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

í…ìŠ¤íŠ¸ ë¶„ëŸ‰ë§Œ ì „ì²´ 12G í…ìŠ¤íŠ¸ ì¤‘ ì¼ë¶€(144MB)ë¡œ ì¤„ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. 

í•œêµ­ì–´ ë°ì´í„°ì…‹/ì½”í¼ìŠ¤ë¥¼ ì¢€ë” ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” [Korpora](https://github.com/ko-nlp/Korpora) íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

** Updates on 2020.09.08 **

Github Releaseë¥¼ í†µí•´ í•™ìŠµ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì˜€ìŠµë‹ˆë‹¤.

ë‹¤ë§Œ í•œ íŒŒì¼ë‹¹ 2GB ì´ë‚´ì˜ ì œì•½ìœ¼ë¡œ ì¸í•´ ë¶„í• ì••ì¶•ë˜ì–´ìˆìŠµë‹ˆë‹¤.

ì•„ë˜ ë§í¬ë¥¼ í†µí•´ ë°›ì•„ì£¼ì„¸ìš”. (ê°€ì… ì—†ì´ ë°›ì„ ìˆ˜ ìˆì–´ìš”. ë¶„í• ì••ì¶•)

ë§Œì•½ í•œ íŒŒì¼ë¡œ ë°›ê³ ì‹¶ìœ¼ì‹œê±°ë‚˜/Kaggleì—ì„œ ë°ì´í„°ë¥¼ ì‚´í´ë³´ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ ì•„ë˜ì˜ ìºê¸€ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ì£¼ì„¸ìš”.

- Githubë¦´ë¦¬ì¦ˆ: https://github.com/Beomi/KcBERT/releases/tag/TrainData_v1

** Updates on 2020.08.22 **

Pretrain Dataset ê³µê°œ

- ìºê¸€: https://www.kaggle.com/junbumlee/kcbert-pretraining-corpus-korean-news-comments (í•œ íŒŒì¼ë¡œ ë°›ì„ ìˆ˜ ìˆì–´ìš”. ë‹¨ì¼íŒŒì¼)

Kaggleì— í•™ìŠµì„ ìœ„í•´ ì •ì œí•œ(ì•„ë˜ `clean`ì²˜ë¦¬ë¥¼ ê±°ì¹œ) Datasetì„ ê³µê°œí•˜ì˜€ìŠµë‹ˆë‹¤!

ì§ì ‘ ë‹¤ìš´ë°›ìœ¼ì…”ì„œ ë‹¤ì–‘í•œ Taskì— í•™ìŠµì„ ì§„í–‰í•´ë³´ì„¸ìš” :) 

---

ê³µê°œëœ í•œêµ­ì–´ BERTëŠ” ëŒ€ë¶€ë¶„ í•œêµ­ì–´ ìœ„í‚¤, ë‰´ìŠ¤ ê¸°ì‚¬, ì±… ë“± ì˜ ì •ì œëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì…ë‹ˆë‹¤. í•œí¸, ì‹¤ì œë¡œ NSMCì™€ ê°™ì€ ëŒ“ê¸€í˜• ë°ì´í„°ì…‹ì€ ì •ì œë˜ì§€ ì•Šì•˜ê³  êµ¬ì–´ì²´ íŠ¹ì§•ì— ì‹ ì¡°ì–´ê°€ ë§ìœ¼ë©°, ì˜¤íƒˆì ë“± ê³µì‹ì ì¸ ê¸€ì“°ê¸°ì—ì„œ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” í‘œí˜„ë“¤ì´ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•©ë‹ˆë‹¤.

KcBERTëŠ” ìœ„ì™€ ê°™ì€ íŠ¹ì„±ì˜ ë°ì´í„°ì…‹ì— ì ìš©í•˜ê¸° ìœ„í•´, ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€ì„ ìˆ˜ì§‘í•´, í† í¬ë‚˜ì´ì €ì™€ BERTëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•œ Pretrained BERT ëª¨ë¸ì…ë‹ˆë‹¤.

KcBERTëŠ” Huggingfaceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ê°„í¸íˆ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë³„ë„ì˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)

## KcBERT Performance

- Finetune ì½”ë“œëŠ” https://github.com/Beomi/KcBERT-finetune ì—ì„œ ì°¾ì•„ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

|                       | Size<br/>(ìš©ëŸ‰)  | **NSMC**<br/>(acc) | **Naver NER**<br/>(F1) | **PAWS**<br/>(acc) | **KorNLI**<br/>(acc) | **KorSTS**<br/>(spearman) | **Question Pair**<br/>(acc) | **KorQuaD (Dev)**<br/>(EM/F1) |
| :-------------------- | :---: | :----------------: | :--------------------: | :----------------: | :------------------: | :-----------------------: | :-------------------------: | :---------------------------: |
| KcBERT-Base                | 417M  |       89.62        |         84.34          |       66.95        |        74.85         |           75.57           |            93.93            |         60.25 / 84.39         |
| KcBERT-Large                | 1.2G  |       **90.68**        |         85.53          |       70.15        |        76.99         |           77.49           |            94.06            |         62.16 / 86.64          |
| KoBERT                | 351M  |       89.63        |         86.11          |       80.65        |        79.00         |           79.64           |            93.93            |         52.81 / 80.27         |
| XLM-Roberta-Base      | 1.03G |       89.49        |         86.26          |       82.95        |        79.92         |           79.09           |            93.53            |         64.70 / 88.94         |
| HanBERT               | 614M  |       90.16        |       **87.31**        |       82.40        |      **80.89**       |           83.33           |            94.19            |         78.74 / 92.02         |
| KoELECTRA-Base    | 423M  |     **90.21**      |         86.87          |       81.90        |        80.85         |           83.21           |            94.20            |         61.10 / 89.59         |
| KoELECTRA-Base-v2 | 423M  |       89.70        |         87.02          |     **83.90**      |        80.61         |         **84.30**         |          **94.72**          |       **84.34 / 92.58**       |
| DistilKoBERT           | 108M |       88.41        |         84.13          |       62.55        |        70.55         |           73.21           |            92.48            |         54.12 / 77.80         |


\*HanBERTì˜ SizeëŠ” Bert Modelê³¼ Tokenizer DBë¥¼ í•©ì¹œ ê²ƒì…ë‹ˆë‹¤.

\***configì˜ ì„¸íŒ…ì„ ê·¸ëŒ€ë¡œ í•˜ì—¬ ëŒë¦° ê²°ê³¼ì´ë©°, hyperparameter tuningì„ ì¶”ê°€ì ìœ¼ë¡œ í•  ì‹œ ë” ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

## How to use

### Requirements

- `pytorch <= 1.8.0`
- `transformers ~= 3.0.1`
  - `transformers ~= 4.0.0` ë„ í˜¸í™˜ë©ë‹ˆë‹¤.
- `emoji ~= 0.6.0`
- `soynlp ~= 0.0.493`

```python
from transformers import AutoTokenizer, AutoModelWithLMHead

# Base Model (108M)

tokenizer = AutoTokenizer.from_pretrained("beomi/kcbert-base")

model = AutoModelWithLMHead.from_pretrained("beomi/kcbert-base")

# Large Model (334M)

tokenizer = AutoTokenizer.from_pretrained("beomi/kcbert-large")

model = AutoModelWithLMHead.from_pretrained("beomi/kcbert-large")
```

### Pretrain & Finetune Colab ë§í¬ ëª¨ìŒ 

#### Pretrain Data

- [ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ(Kaggle, ë‹¨ì¼íŒŒì¼, ë¡œê·¸ì¸ í•„ìš”)](https://www.kaggle.com/junbumlee/kcbert-pretraining-corpus-korean-news-comments)
- [ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ(Github, ì••ì¶• ì—¬ëŸ¬íŒŒì¼, ë¡œê·¸ì¸ ë¶ˆí•„ìš”)](https://github.com/Beomi/KcBERT/releases/tag/TrainData_v1)

#### Pretrain Code

Colabì—ì„œ TPUë¡œ KcBERT Pretrain í•´ë³´ê¸°: <a href="https://colab.research.google.com/drive/1lYBYtaXqt9S733OXdXvrvC09ysKFN30W">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

#### Finetune Samples

**KcBERT-Base** NSMC Finetuning with PyTorch-Lightning (Colab) <a href="https://colab.research.google.com/drive/1fn4sVJ82BrrInjq6y5655CYPP-1UKCLb?usp=sharing">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

**KcBERT-Large** NSMC Finetuning with PyTorch-Lightning (Colab) <a href="https://colab.research.google.com/drive/1dFC0FL-521m7CL_PSd8RLKq67jgTJVhL?usp=sharing">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

> ìœ„ ë‘ ì½”ë“œëŠ” Pretrain ëª¨ë¸(base, large)ì™€ batch sizeë§Œ ë‹¤ë¥¼ ë¿, ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ì™„ì „íˆ ë™ì¼í•©ë‹ˆë‹¤.

## Train Data & Preprocessing

### Raw Data

í•™ìŠµ ë°ì´í„°ëŠ” 2019.01.01 ~ 2020.06.15 ì‚¬ì´ì— ì‘ì„±ëœ **ëŒ“ê¸€ ë§ì€ ë‰´ìŠ¤** ê¸°ì‚¬ë“¤ì˜ **ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€**ì„ ëª¨ë‘ ìˆ˜ì§‘í•œ ë°ì´í„°ì…ë‹ˆë‹¤.

ë°ì´í„° ì‚¬ì´ì¦ˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œì‹œ **ì•½ 15.4GBì´ë©°, 1ì–µ1ì²œë§Œê°œ ì´ìƒì˜ ë¬¸ì¥**ìœ¼ë¡œ ì´ë¤„ì ¸ ìˆìŠµë‹ˆë‹¤.

### Preprocessing

PLM í•™ìŠµì„ ìœ„í•´ì„œ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•œ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. í•œê¸€ ë° ì˜ì–´, íŠ¹ìˆ˜ë¬¸ì, ê·¸ë¦¬ê³  ì´ëª¨ì§€(ğŸ¥³)ê¹Œì§€!

   ì •ê·œí‘œí˜„ì‹ì„ í†µí•´ í•œê¸€, ì˜ì–´, íŠ¹ìˆ˜ë¬¸ìë¥¼ í¬í•¨í•´ Emojiê¹Œì§€ í•™ìŠµ ëŒ€ìƒì— í¬í•¨í–ˆìŠµë‹ˆë‹¤.

   í•œí¸, í•œê¸€ ë²”ìœ„ë¥¼ `ã„±-ã…ê°€-í£` ìœ¼ë¡œ ì§€ì •í•´ `ã„±-í£` ë‚´ì˜ í•œìë¥¼ ì œì™¸í–ˆìŠµë‹ˆë‹¤. 

2. ëŒ“ê¸€ ë‚´ ì¤‘ë³µ ë¬¸ìì—´ ì¶•ì•½

   `ã…‹ã…‹ã…‹ã…‹ã…‹`ì™€ ê°™ì´ ì¤‘ë³µëœ ê¸€ìë¥¼ `ã…‹ã…‹`ì™€ ê°™ì€ ê²ƒìœ¼ë¡œ í•©ì³¤ìŠµë‹ˆë‹¤.

3. Cased Model

   KcBERTëŠ” ì˜ë¬¸ì— ëŒ€í•´ì„œëŠ” ëŒ€ì†Œë¬¸ìë¥¼ ìœ ì§€í•˜ëŠ” Cased modelì…ë‹ˆë‹¤.

4. ê¸€ì ë‹¨ìœ„ 10ê¸€ì ì´í•˜ ì œê±°

   10ê¸€ì ë¯¸ë§Œì˜ í…ìŠ¤íŠ¸ëŠ” ë‹¨ì¼ ë‹¨ì–´ë¡œ ì´ë¤„ì§„ ê²½ìš°ê°€ ë§ì•„ í•´ë‹¹ ë¶€ë¶„ì„ ì œì™¸í–ˆìŠµë‹ˆë‹¤.

5. ì¤‘ë³µ ì œê±°

   ì¤‘ë³µì ìœ¼ë¡œ ì“°ì¸ ëŒ“ê¸€ì„ ì œê±°í•˜ê¸° ìœ„í•´ ì¤‘ë³µ ëŒ“ê¸€ì„ í•˜ë‚˜ë¡œ í•©ì³¤ìŠµë‹ˆë‹¤.

ì´ë¥¼ í†µí•´ ë§Œë“  ìµœì¢… í•™ìŠµ ë°ì´í„°ëŠ” **12.5GB, 8.9ì²œë§Œê°œ ë¬¸ì¥**ì…ë‹ˆë‹¤.

ì•„ë˜ ëª…ë ¹ì–´ë¡œ pipë¡œ ì„¤ì¹˜í•œ ë’¤, ì•„ë˜ cleaní•¨ìˆ˜ë¡œ í´ë¦¬ë‹ì„ í•˜ë©´ Downstream taskì—ì„œ ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤. (`[UNK]` ê°ì†Œ)

```bash
pip install soynlp emoji
```

ì•„ë˜ `clean` í•¨ìˆ˜ë¥¼ Text dataì— ì‚¬ìš©í•´ì£¼ì„¸ìš”.

```python
import re
import emoji
from soynlp.normalizer import repeat_normalize

emojis = ''.join(emoji.UNICODE_EMOJI.keys())
pattern = re.compile(f'[^ .,?!/@$%~ï¼…Â·âˆ¼()\x00-\x7Fã„±-ã…£ê°€-í£{emojis}]+')
url_pattern = re.compile(
    r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)')

def clean(x):
    x = pattern.sub(' ', x)
    x = url_pattern.sub('', x)
    x = x.strip()
    x = repeat_normalize(x, num_repeats=2)
    return x
```

### Cleaned Data (Released on Kaggle)

ì›ë³¸ ë°ì´í„°ë¥¼ ìœ„ `clean`í•¨ìˆ˜ë¡œ ì •ì œí•œ 12GBë¶„ëŸ‰ì˜ txt íŒŒì¼ì„ ì•„ë˜ Kaggle Datasetì—ì„œ ë‹¤ìš´ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤ :)

https://www.kaggle.com/junbumlee/kcbert-pretraining-corpus-korean-news-comments


## Tokenizer Train

TokenizerëŠ” Huggingfaceì˜ [Tokenizers](https://github.com/huggingface/tokenizers) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

ê·¸ ì¤‘ `BertWordPieceTokenizer` ë¥¼ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í–ˆê³ , Vocab SizeëŠ” `30000`ìœ¼ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

Tokenizerë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì—ëŠ” `1/10`ë¡œ ìƒ˜í”Œë§í•œ ë°ì´í„°ë¡œ í•™ìŠµì„ ì§„í–‰í–ˆê³ , ë³´ë‹¤ ê³¨ê³ ë£¨ ìƒ˜í”Œë§í•˜ê¸° ìœ„í•´ ì¼ìë³„ë¡œ stratifyë¥¼ ì§€ì •í•œ ë’¤ í–‘ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

## BERT Model Pretrain

- KcBERT Base config

```json
{
    "max_position_embeddings": 300,
    "hidden_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "initializer_range": 0.02,
    "num_hidden_layers": 12,
    "type_vocab_size": 2,
    "vocab_size": 30000,
    "hidden_size": 768,
    "attention_probs_dropout_prob": 0.1,
    "directionality": "bidi",
    "num_attention_heads": 12,
    "intermediate_size": 3072,
    "architectures": [
        "BertForMaskedLM"
    ],
    "model_type": "bert"
}
```

- KcBERT Large config

```json
{
    "type_vocab_size": 2,
    "initializer_range": 0.02,
    "max_position_embeddings": 300,
    "vocab_size": 30000,
    "hidden_size": 1024,
    "hidden_dropout_prob": 0.1,
    "model_type": "bert",
    "directionality": "bidi",
    "pad_token_id": 0,
    "layer_norm_eps": 1e-12,
    "hidden_act": "gelu",
    "num_hidden_layers": 24,
    "num_attention_heads": 16,
    "attention_probs_dropout_prob": 0.1,
    "intermediate_size": 4096,
    "architectures": [
        "BertForMaskedLM"
    ]
}
```

BERT Model ConfigëŠ” Base, Large ê¸°ë³¸ ì„¸íŒ…ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. (MLM 15% ë“±)

TPU `v3-8` ì„ ì´ìš©í•´ ê°ê° 3ì¼, Nì¼(LargeëŠ” í•™ìŠµ ì§„í–‰ ì¤‘)ì„ ì§„í–‰í–ˆê³ , í˜„ì¬ Huggingfaceì— ê³µê°œëœ ëª¨ë¸ì€ 1m(100ë§Œ) stepì„ í•™ìŠµí•œ ckptê°€ ì—…ë¡œë“œ ë˜ì–´ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ í•™ìŠµ LossëŠ” Stepì— ë”°ë¼ ì´ˆê¸° 200kì— ê°€ì¥ ë¹ ë¥´ê²Œ Lossê°€ ì¤„ì–´ë“¤ë‹¤ 400kì´í›„ë¡œëŠ” ì¡°ê¸ˆì”© ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- Base Model Loss

![KcBERT-Base Pretraining Loss](./img/image-20200719183852243.38b124.png)

- Large Model Loss

![KcBERT-Large Pretraining Loss](./img/image-20200806160746694.d56fa1.png)

í•™ìŠµì€ GCPì˜ TPU v3-8ì„ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í–ˆê³ , í•™ìŠµ ì‹œê°„ì€ Base Model ê¸°ì¤€ 2.5ì¼ì •ë„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. Large Modelì€ ì•½ 5ì¼ì •ë„ ì§„í–‰í•œ ë’¤ ê°€ì¥ ë‚®ì€ lossë¥¼ ê°€ì§„ ì²´í¬í¬ì¸íŠ¸ë¡œ ì •í–ˆìŠµë‹ˆë‹¤.

## Example

### HuggingFace MASK LM

[HuggingFace kcbert-base ëª¨ë¸](https://huggingface.co/beomi/kcbert-base?text=ì˜¤ëŠ˜ì€+ë‚ ì”¨ê°€+[MASK]) ì—ì„œ ì•„ë˜ì™€ ê°™ì´ í…ŒìŠ¤íŠ¸ í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ "ì¢‹ë„¤ìš”", KcBERT-Base](./img/image-20200719205919389.5670d6.png)

ë¬¼ë¡  [kcbert-large ëª¨ë¸](https://huggingface.co/beomi/kcbert-large?text=ì˜¤ëŠ˜ì€+ë‚ ì”¨ê°€+[MASK]) ì—ì„œë„ í…ŒìŠ¤íŠ¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![image-20200806160624340](./img/image-20200806160624340.58f9be.png)



### NSMC Binary Classification

[ë„¤ì´ë²„ ì˜í™”í‰ ì½”í¼ìŠ¤](https://github.com/e9t/nsmc) ë°ì´í„°ì…‹ì„ ëŒ€ìƒìœ¼ë¡œ Fine Tuningì„ ì§„í–‰í•´ ì„±ëŠ¥ì„ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸í•´ë³´ì•˜ìŠµë‹ˆë‹¤.

Base Modelì„ Fine Tuneí•˜ëŠ” ì½”ë“œëŠ” <a href="https://colab.research.google.com/drive/1fn4sVJ82BrrInjq6y5655CYPP-1UKCLb?usp=sharing">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a> ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Large Modelì„ Fine Tuneí•˜ëŠ” ì½”ë“œëŠ” <a href="https://colab.research.google.com/drive/1dFC0FL-521m7CL_PSd8RLKq67jgTJVhL?usp=sharing">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a> ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- GPUëŠ” P100 x1ëŒ€ ê¸°ì¤€ 1epochì— 2-3ì‹œê°„, TPUëŠ” 1epochì— 1ì‹œê°„ ë‚´ë¡œ ì†Œìš”ë©ë‹ˆë‹¤.
- GPU RTX Titan x4ëŒ€ ê¸°ì¤€ 30ë¶„/epoch ì†Œìš”ë©ë‹ˆë‹¤.
- ì˜ˆì‹œ ì½”ë“œëŠ” [pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning)ìœ¼ë¡œ ê°œë°œí–ˆìŠµë‹ˆë‹¤.

#### ì‹¤í—˜ê²°ê³¼

- KcBERT-Base Model ì‹¤í—˜ê²°ê³¼: Val acc `.8905`

  ![KcBERT Base finetune on NSMC](./img/image-20200719201102895.ddbdfc.png)

- KcBERT-Large Model ì‹¤í—˜ ê²°ê³¼: Val acc `.9089`

  ![image-20200806190242834](./img/image-20200806190242834.56d6ee.png)

> ë” ë‹¤ì–‘í•œ Downstream Taskì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ê³  ê³µê°œí•  ì˜ˆì •ì…ë‹ˆë‹¤.

## ì¸ìš©í‘œê¸°/Citation

KcBERTë¥¼ ì¸ìš©í•˜ì‹¤ ë•ŒëŠ” ì•„ë˜ ì–‘ì‹ì„ í†µí•´ ì¸ìš©í•´ì£¼ì„¸ìš”.

```
@inproceedings{lee2020kcbert,
  title = {{KcBERT}: Korean Comments BERT,
  author = {Junbum Lee},
  booktitle = {Proceedings of the 32nd Annual Conference on Human and Cognitive Language Technology, pp. 437-440.},
  year = {2020},
  url = {https://sites.google.com/view/hclt2020}
}
```

- ë…¼ë¬¸ì§‘ ë‹¤ìš´ë¡œë“œ ë§í¬: http://hclt.kr/dwn/?v=bG5iOmNvbmZlcmVuY2U7aWR4OjMy (*í˜¹ì€ http://hclt.kr/symp/?lnb=conference )

## Acknowledgement

KcBERT Modelì„ í•™ìŠµí•˜ëŠ” GCP/TPU í™˜ê²½ì€ [TFRC](https://www.tensorflow.org/tfrc?hl=ko) í”„ë¡œê·¸ë¨ì˜ ì§€ì›ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.

ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ë§ì€ ì¡°ì–¸ì„ ì£¼ì‹  [Monologg](https://github.com/monologg/) ë‹˜ ê°ì‚¬í•©ë‹ˆë‹¤ :)

## Reference

### Github Repos

- [BERT by Google](https://github.com/google-research/bert)
- [KoBERT by SKT](https://github.com/SKTBrain/KoBERT)
- [KoELECTRA by Monologg](https://github.com/monologg/KoELECTRA/)

- [Transformers by Huggingface](https://github.com/huggingface/transformers)
- [Tokenizers by Hugginface](https://github.com/huggingface/tokenizers)

### Papers

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

### Blogs

- [Monologgë‹˜ì˜ KoELECTRA í•™ìŠµê¸°](https://monologg.kr/categories/NLP/ELECTRA/)
- [Colabì—ì„œ TPUë¡œ BERT ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¤ê¸° - Tensorflow/Google ver.](https://beomi.github.io/2020/02/26/Train-BERT-from-scratch-on-colab-TPU-Tensorflow-ver/)

