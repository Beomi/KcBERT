# KcBERT: Korean comments BERT

ê³µê°œëœ í•œêµ­ì–´ BERTëŠ” ëŒ€ë¶€ë¶„ í•œêµ­ì–´ ìœ„í‚¤, ë‰´ìŠ¤ ê¸°ì‚¬, ì±… ë“± ì˜ ì •ì œëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì…ë‹ˆë‹¤. í•œí¸, ì‹¤ì œë¡œ NSMCì™€ ê°™ì€ ëŒ“ê¸€í˜• ë°ì´í„°ì…‹ì€ ì •ì œë˜ì§€ ì•Šì•˜ê³  êµ¬ì–´ì²´ íŠ¹ì§•ì— ì‹ ì¡°ì–´ê°€ ë§ìœ¼ë©°, ì˜¤íƒˆì ë“± ê³µì‹ì ì¸ ê¸€ì“°ê¸°ì—ì„œ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” í‘œí˜„ë“¤ì´ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•©ë‹ˆë‹¤.

KcBERTëŠ” ìœ„ì™€ ê°™ì€ íŠ¹ì„±ì˜ ë°ì´í„°ì…‹ì— ì ìš©í•˜ê¸° ìœ„í•´, ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€ì„ ìˆ˜ì§‘í•´, í† í¬ë‚˜ì´ì €ì™€ BERTëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•œ Pretrained BERT ëª¨ë¸ì…ë‹ˆë‹¤.

KcBERTëŠ” Huggingfaceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ê°„í¸íˆ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë³„ë„ì˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)

## How to use

### Requirements

- `pytorch ~= 1.5.1`

- `transformers ~= 3.0.1`
- `emoji ~= 0.6.0`
- `soynlp ~= 0.0.493`

```python
from transformers import AutoTokenizer, AutoModelWithLMHead

# Base Model (108M)

tokenizer = AutoTokenizer.from_pretrained("beomi/kcbert-base")

model = AutoModelWithLMHead.from_pretrained("beomi/kcbert-base")

# Large Model (334M)

tokenizer = AutoTokenizer.from_pretrained("beomi/kcbert-large")

model = AutoModelWithLMHead.from_pretrained("beomi/kcbert-large")
```

## Train Data & Preprocessing

### Raw Data

í•™ìŠµ ë°ì´í„°ëŠ” 2019.01.01 ~ 2020.06.15 ì‚¬ì´ì— ì‘ì„±ëœ **ëŒ“ê¸€ ë§ì€ ë‰´ìŠ¤** ê¸°ì‚¬ë“¤ì˜ **ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€**ì„ ëª¨ë‘ ìˆ˜ì§‘í•œ ë°ì´í„°ì…ë‹ˆë‹¤.

ë°ì´í„° ì‚¬ì´ì¦ˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œì‹œ **ì•½ 15.4GBì´ë©°, 1ì–µ1ì²œë§Œê°œ ì´ìƒì˜ ë¬¸ì¥**ìœ¼ë¡œ ì´ë¤„ì ¸ ìˆìŠµë‹ˆë‹¤.

### Preprocessing

PLM í•™ìŠµì„ ìœ„í•´ì„œ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•œ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. í•œê¸€ ë° ì˜ì–´, íŠ¹ìˆ˜ë¬¸ì, ê·¸ë¦¬ê³  ì´ëª¨ì§€(ğŸ¥³)ê¹Œì§€!

   ì •ê·œí‘œí˜„ì‹ì„ í†µí•´ í•œê¸€, ì˜ì–´, íŠ¹ìˆ˜ë¬¸ìë¥¼ í¬í•¨í•´ Emojiê¹Œì§€ í•™ìŠµ ëŒ€ìƒì— í¬í•¨í–ˆìŠµë‹ˆë‹¤.

   í•œí¸, í•œê¸€ ë²”ìœ„ë¥¼ `ã„±-ã…ê°€-í£` ìœ¼ë¡œ ì§€ì •í•´ `ã„±-í£` ë‚´ì˜ í•œìë¥¼ ì œì™¸í–ˆìŠµë‹ˆë‹¤. 

2. ëŒ“ê¸€ ë‚´ ì¤‘ë³µ ë¬¸ìì—´ ì¶•ì•½

   `ã…‹ã…‹ã…‹ã…‹ã…‹`ì™€ ê°™ì´ ì¤‘ë³µëœ ê¸€ìë¥¼ `ã…‹ã…‹`ì™€ ê°™ì€ ê²ƒìœ¼ë¡œ í•©ì³¤ìŠµë‹ˆë‹¤.

3. Cased Model

   KcBERTëŠ” ì˜ë¬¸ì— ëŒ€í•´ì„œëŠ” ëŒ€ì†Œë¬¸ìë¥¼ ìœ ì§€í•˜ëŠ” Cased modelì…ë‹ˆë‹¤.

4. ê¸€ì ë‹¨ìœ„ 10ê¸€ì ì´í•˜ ì œê±°

   10ê¸€ì ë¯¸ë§Œì˜ í…ìŠ¤íŠ¸ëŠ” ë‹¨ì¼ ë‹¨ì–´ë¡œ ì´ë¤„ì§„ ê²½ìš°ê°€ ë§ì•„ í•´ë‹¹ ë¶€ë¶„ì„ ì œì™¸í–ˆìŠµë‹ˆë‹¤.

5. ì¤‘ë³µ ì œê±°

   ì¤‘ë³µì ìœ¼ë¡œ ì“°ì¸ ëŒ“ê¸€ì„ ì œê±°í•˜ê¸° ìœ„í•´ ì¤‘ë³µ ëŒ“ê¸€ì„ í•˜ë‚˜ë¡œ í•©ì³¤ìŠµë‹ˆë‹¤.

ì´ë¥¼ í†µí•´ ë§Œë“  ìµœì¢… í•™ìŠµ ë°ì´í„°ëŠ” **12.5GB, 8.9ì²œë§Œê°œ ë¬¸ì¥**ì…ë‹ˆë‹¤.

ì•„ë˜ ëª…ë ¹ì–´ë¡œ pipë¡œ ì„¤ì¹˜í•œ ë’¤, ì•„ë˜ cleaní•¨ìˆ˜ë¡œ í´ë¦¬ë‹ì„ í•˜ë©´ Downstream taskì—ì„œ ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤. (`[UNK]` ê°ì†Œ)

```bash
pip install soynlp emoji
```

ì•„ë˜ `clean` í•¨ìˆ˜ë¥¼ Text dataì— ì‚¬ìš©í•´ì£¼ì„¸ìš”.

```python
import re
import emoji
from soynlp.normalizer import repeat_normalize

emojis = ''.join(emoji.UNICODE_EMOJI.keys())
pattern = re.compile(f'[^ .,?!/@$%~ï¼…Â·âˆ¼()\x00-\x7Fã„±-í£{emojis}]+')
url_pattern = re.compile(
    r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)')

def clean(x):
    x = pattern.sub(' ', x)
    x = url_pattern.sub('', x)
    x = x.strip()
    x = repeat_normalize(x, num_repeats=2)
    return x
```

## Tokenizer Train

TokenizerëŠ” Huggingfaceì˜ [Tokenizers](https://github.com/huggingface/tokenizers) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

ê·¸ ì¤‘ `BertWordPieceTokenizer` ë¥¼ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í–ˆê³ , Vocab SizeëŠ” `30000`ìœ¼ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

Tokenizerë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì—ëŠ” `1/10`ë¡œ ìƒ˜í”Œë§í•œ ë°ì´í„°ë¡œ í•™ìŠµì„ ì§„í–‰í–ˆê³ , ë³´ë‹¤ ê³¨ê³ ë£¨ ìƒ˜í”Œë§í•˜ê¸° ìœ„í•´ ì¼ìë³„ë¡œ stratifyë¥¼ ì§€ì •í•œ ë’¤ í–‘ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

## BERT Model Pretrain

- KcBERT Base config

```json
{
    "max_position_embeddings": 300,
    "hidden_dropout_prob": 0.1,
    "pooler_size_per_head": 128,
    "hidden_act": "gelu",
    "initializer_range": 0.02,
    "num_hidden_layers": 12,
    "pooler_num_attention_heads": 12,
    "type_vocab_size": 2,
    "vocab_size": 30000,
    "hidden_size": 768,
    "attention_probs_dropout_prob": 0.1,
    "directionality": "bidi",
    "num_attention_heads": 12,
    "pooler_fc_size": 768,
    "pooler_type": "first_token_transform",
    "pooler_num_fc_layers": 3,
    "intermediate_size": 3072,
    "architectures": [
        "BertForMaskedLM"
    ],
    "model_type": "bert"
}
```

- KcBERT Large config

```json
{
    "type_vocab_size": 2,
    "initializer_range": 0.02,
    "max_position_embeddings": 300,
    "vocab_size": 30000,
    "hidden_size": 1024,
    "hidden_dropout_prob": 0.1,
    "model_type": "bert",
    "directionality": "bidi",
    "pooler_num_attention_heads": 12,
    "pooler_fc_size": 768,
    "pad_token_id": 0,
    "pooler_type": "first_token_transform",
    "layer_norm_eps": 1e-12,
    "hidden_act": "gelu",
    "num_hidden_layers": 24,
    "pooler_num_fc_layers": 3,
    "num_attention_heads": 16,
    "pooler_size_per_head": 128,
    "attention_probs_dropout_prob": 0.1,
    "intermediate_size": 4096,
    "architectures": [
        "BertForMaskedLM"
    ]
}
```

BERT Model ConfigëŠ” Base, Large ê¸°ë³¸ ì„¸íŒ…ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. (MLM 15% ë“±)

TPU `v3-8` ì„ ì´ìš©í•´ ê°ê° 3ì¼, Nì¼(LargeëŠ” í•™ìŠµ ì§„í–‰ ì¤‘)ì„ ì§„í–‰í–ˆê³ , í˜„ì¬ Huggingfaceì— ê³µê°œëœ ëª¨ë¸ì€ 1m(100ë§Œ) stepì„ í•™ìŠµí•œ ckptê°€ ì—…ë¡œë“œ ë˜ì–´ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ í•™ìŠµ LossëŠ” Stepì— ë”°ë¼ ì´ˆê¸° 200kì— ê°€ì¥ ë¹ ë¥´ê²Œ Lossê°€ ì¤„ì–´ë“¤ë‹¤ 400kì´í›„ë¡œëŠ” ì¡°ê¸ˆì”© ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- Base Model Loss

![KcBERT-Base Pretraining Loss](./img/image-20200719183852243.38b124.png)

- Large Model Loss

![KcBERT-Large Pretraining Loss](./img/image-20200806160746694.d56fa1.png)

í•™ìŠµì€ GCPì˜ TPU v3-8ì„ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í–ˆê³ , í•™ìŠµ ì‹œê°„ì€ Base Model ê¸°ì¤€ 2.5ì¼ì •ë„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. Large Modelì€ ì•½ 5ì¼ì •ë„ ì§„í–‰í•œ ë’¤ ê°€ì¥ ë‚®ì€ lossë¥¼ ê°€ì§„ ì²´í¬í¬ì¸íŠ¸ë¡œ ì •í–ˆìŠµë‹ˆë‹¤.

## Example

### HuggingFace MASK LM

[HuggingFace kcbert-base ëª¨ë¸](https://huggingface.co/beomi/kcbert-base?text=ì˜¤ëŠ˜ì€+ë‚ ì”¨ê°€+[MASK]) ì—ì„œ ì•„ë˜ì™€ ê°™ì´ í…ŒìŠ¤íŠ¸ í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ "ì¢‹ë„¤ìš”", KcBERT-Base](./img/image-20200719205919389.5670d6.png)

ë¬¼ë¡  [kcbert-large ëª¨ë¸](https://huggingface.co/beomi/kcbert-large?text=ì˜¤ëŠ˜ì€+ë‚ ì”¨ê°€+[MASK]) ì—ì„œë„ í…ŒìŠ¤íŠ¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![image-20200806160624340](./img/image-20200806160624340.58f9be.png)



### NSMC Binary Classification (Base Acc: `.89048`, Large Acc: `.9070`)

[ë„¤ì´ë²„ ì˜í™”í‰ ì½”í¼ìŠ¤](https://github.com/e9t/nsmc) ë°ì´í„°ì…‹ì„ ëŒ€ìƒìœ¼ë¡œ Fine Tuningì„ ì§„í–‰í•´ ì„±ëŠ¥ì„ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸í•´ë³´ì•˜ìŠµë‹ˆë‹¤.

> - Base Modelì„ Fine Tuneí•˜ëŠ” ì½”ë“œëŠ” [ì´ Colab ë§í¬](https://colab.research.google.com/gist/Beomi/c26cf67f9fb717d81141c579635816b2/kcbert-nsmc.ipynb)ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> - Large Modelì„ Fine Tuneí•˜ëŠ” ì½”ë“œëŠ” [GPUë²„ì „ Colab ë§í¬](https://colab.research.google.com/drive/1dFC0FL-521m7CL_PSd8RLKq67jgTJVhL?usp=sharing) ì™€ TPUë²„ì „ Colab ë§í¬(ê³µê°œì˜ˆì •, ì‘ì—…ì¤‘)ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
>   - GPUëŠ” P100 x1ëŒ€ ê¸°ì¤€ 1epochì— 2-3ì‹œê°„, TPUëŠ” 1epochì— 1ì‹œê°„ ë‚´ë¡œ ì†Œìš”ë©ë‹ˆë‹¤.
>     - GPU RTX Titan x4ëŒ€ ê¸°ì¤€ 30ë¶„/epoch ì†Œìš”ë©ë‹ˆë‹¤.
>   - Large Model ì˜ˆì‹œ ì½”ë“œëŠ” [pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning)ìœ¼ë¡œ ê°œë°œí–ˆìŠµë‹ˆë‹¤.

- KcBERT-Base Model ì‹¤í—˜ê²°ê³¼: Val acc `.8905`

  ![KcBERT Base finetune on NSMC](./img/image-20200719201102895.ddbdfc.png)

- KcBERT-Large Model ì‹¤í—˜ ê²°ê³¼: Val acc `.9089`

  ![image-20200806190242834](./img/image-20200806190242834.56d6ee.png)

> ë” ë‹¤ì–‘í•œ Downstream Taskì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ê³  ê³µê°œí•  ì˜ˆì •ì…ë‹ˆë‹¤.



## Acknowledgement

KcBERT Modelì„ í•™ìŠµí•˜ëŠ” GCP/TPU í™˜ê²½ì€ [TFRC](https://www.tensorflow.org/tfrc?hl=ko) í”„ë¡œê·¸ë¨ì˜ ì§€ì›ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.

ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ë§ì€ ì¡°ì–¸ì„ ì£¼ì‹  [Monologg](https://github.com/monologg/) ë‹˜ ê°ì‚¬í•©ë‹ˆë‹¤ :)

## Reference

### Github Repos

- [BERT by Google](https://github.com/google-research/bert)
- [KoBERT by SKT](https://github.com/SKTBrain/KoBERT)
- [KoELECTRA by Monologg](https://github.com/monologg/KoELECTRA/)

- [Transformers by Huggingface](https://github.com/huggingface/transformers)
- [Tokenizers by Hugginface](https://github.com/huggingface/tokenizers)

### Papers

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

### Blogs

- [Monologgë‹˜ì˜ KoELECTRA í•™ìŠµê¸°](https://monologg.kr/categories/NLP/ELECTRA/)
- [Colabì—ì„œ TPUë¡œ BERT ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¤ê¸° - Tensorflow/Google ver.](/2020/02/26/Train-BERT-from-scratch-on-colab-TPU-Tensorflow-ver/)

