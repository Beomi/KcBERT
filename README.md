# KcBERT: Korean comments BERT

ê³µê°œëœ í•œêµ­ì–´ BERTëŠ” ëŒ€ë¶€ë¶„ í•œêµ­ì–´ ìœ„í‚¤, ë‰´ìŠ¤ ê¸°ì‚¬, ì±… ë“± ì˜ ì •ì œëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì…ë‹ˆë‹¤. í•œí¸, ì‹¤ì œë¡œ NSMCì™€ ê°™ì€ ëŒ“ê¸€í˜• ë°ì´í„°ì…‹ì€ ì •ì œë˜ì§€ ì•Šì•˜ê³  êµ¬ì–´ì²´ íŠ¹ì§•ì— ì‹ ì¡°ì–´ê°€ ë§ìœ¼ë©°, ì˜¤íƒˆì ë“± ê³µì‹ì ì¸ ê¸€ì“°ê¸°ì—ì„œ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” í‘œí˜„ë“¤ì´ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•©ë‹ˆë‹¤.

KcBERTëŠ” ìœ„ì™€ ê°™ì€ íŠ¹ì„±ì˜ ë°ì´í„°ì…‹ì— ì ìš©í•˜ê¸° ìœ„í•´, ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€ì„ ìˆ˜ì§‘í•´, í† í¬ë‚˜ì´ì €ì™€ BERTëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµí•œ Pretrained BERT ëª¨ë¸ì…ë‹ˆë‹¤.

KcBERTëŠ” Huggingfaceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ê°„í¸íˆ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë³„ë„ì˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)


## How to use

> Huggingface Model Page: https://huggingface.co/beomi/kcbert-base

- `pytorch ~= 1.5.1`
- `transformers ~= 3.0.1`

```python
from transformers import AutoTokenizer, AutoModelWithLMHead

tokenizer = AutoTokenizer.from_pretrained("beomi/kcbert-base")

model = AutoModelWithLMHead.from_pretrained("beomi/kcbert-base")

# BERT Large Model ê³µê°œ ì˜ˆì • 

# tokenizer = AutoTokenizer.from_pretrained("beomi/kcbert-large")

# model = AutoModelWithLMHead.from_pretrained("beomi/kcbert-large")
```

## Train Data & Preprocessing

### Raw Data

í•™ìŠµ ë°ì´í„°ëŠ” 2019.01.01 ~ 2020.06.15 ì‚¬ì´ì— ì‘ì„±ëœ **ëŒ“ê¸€ ë§ì€ ë‰´ìŠ¤** ê¸°ì‚¬ë“¤ì˜ **ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€**ì„ ëª¨ë‘ ìˆ˜ì§‘í•œ ë°ì´í„°ì…ë‹ˆë‹¤.

ë°ì´í„° ì‚¬ì´ì¦ˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œì‹œ **ì•½ 15.4GBì´ë©°, 1ì–µ1ì²œë§Œê°œ ì´ìƒì˜ ë¬¸ì¥**ìœ¼ë¡œ ì´ë¤„ì ¸ ìˆìŠµë‹ˆë‹¤.

### Preprocessing

PLM í•™ìŠµì„ ìœ„í•´ì„œ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•œ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. í•œê¸€ ë° ì˜ì–´, íŠ¹ìˆ˜ë¬¸ì, ê·¸ë¦¬ê³  ì´ëª¨ì§€(ğŸ¥³)ê¹Œì§€!

   ì •ê·œí‘œí˜„ì‹ì„ í†µí•´ í•œê¸€, ì˜ì–´, íŠ¹ìˆ˜ë¬¸ìë¥¼ í¬í•¨í•´ Emojiê¹Œì§€ í•™ìŠµ ëŒ€ìƒì— í¬í•¨í–ˆìŠµë‹ˆë‹¤.

   í•œí¸, í•œê¸€ ë²”ìœ„ë¥¼ `ã„±-ã…ê°€-í£` ìœ¼ë¡œ ì§€ì •í•´ `ã„±-í£` ë‚´ì˜ í•œìë¥¼ ì œì™¸í–ˆìŠµë‹ˆë‹¤. 

2. ëŒ“ê¸€ ë‚´ ì¤‘ë³µ ë¬¸ìì—´ ì¶•ì•½

   `ã…‹ã…‹ã…‹ã…‹ã…‹`ì™€ ê°™ì´ ì¤‘ë³µëœ ê¸€ìë¥¼ `ã…‹ã…‹`ì™€ ê°™ì€ ê²ƒìœ¼ë¡œ í•©ì³¤ìŠµë‹ˆë‹¤.

3. Cased Model

   KcBERTëŠ” ì˜ë¬¸ì— ëŒ€í•´ì„œëŠ” ëŒ€ì†Œë¬¸ìë¥¼ ìœ ì§€í•˜ëŠ” Cased modelì…ë‹ˆë‹¤.

4. ê¸€ì ë‹¨ìœ„ 10ê¸€ì ì´í•˜ ì œê±°

   10ê¸€ì ë¯¸ë§Œì˜ í…ìŠ¤íŠ¸ëŠ” ë‹¨ì¼ ë‹¨ì–´ë¡œ ì´ë¤„ì§„ ê²½ìš°ê°€ ë§ì•„ í•´ë‹¹ ë¶€ë¶„ì„ ì œì™¸í–ˆìŠµë‹ˆë‹¤.

5. ì¤‘ë³µ ì œê±°

   ì¤‘ë³µì ìœ¼ë¡œ ì“°ì¸ ëŒ“ê¸€ì„ ì œê±°í•˜ê¸° ìœ„í•´ ì¤‘ë³µ ëŒ“ê¸€ì„ í•˜ë‚˜ë¡œ í•©ì³¤ìŠµë‹ˆë‹¤.

ì´ë¥¼ í†µí•´ ë§Œë“  ìµœì¢… í•™ìŠµ ë°ì´í„°ëŠ” **12.5GB, 8.9ì²œë§Œê°œ ë¬¸ì¥**ì…ë‹ˆë‹¤.


## Tokenizer Train

TokenizerëŠ” Huggingfaceì˜ [Tokenizers](https://github.com/huggingface/tokenizers) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

ê·¸ ì¤‘ `BertWordPieceTokenizer` ë¥¼ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í–ˆê³ , Vocab SizeëŠ” `30000`ìœ¼ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

Tokenizerë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì—ëŠ” `1/10`ë¡œ ìƒ˜í”Œë§í•œ ë°ì´í„°ë¡œ í•™ìŠµì„ ì§„í–‰í–ˆê³ , ë³´ë‹¤ ê³¨ê³ ë£¨ ìƒ˜í”Œë§í•˜ê¸° ìœ„í•´ ì¼ìë³„ë¡œ stratifyë¥¼ ì§€ì •í•œ ë’¤ í–‘ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

## BERT Model Pretrain

```json
{
    "max_position_embeddings": 300,
    "hidden_dropout_prob": 0.1,
    "pooler_size_per_head": 128,
    "hidden_act": "gelu",
    "initializer_range": 0.02,
    "num_hidden_layers": 12,
    "pooler_num_attention_heads": 12,
    "type_vocab_size": 2,
    "vocab_size": 30000,
    "hidden_size": 768,
    "attention_probs_dropout_prob": 0.1,
    "directionality": "bidi",
    "num_attention_heads": 12,
    "pooler_fc_size": 768,
    "pooler_type": "first_token_transform",
    "pooler_num_fc_layers": 3,
    "intermediate_size": 3072,
    "architectures": [
        "BertForMaskedLM"
    ],
    "model_type": "bert"
}
```

BERT Model ConfigëŠ” Base, Large ê¸°ë³¸ ì„¸íŒ…ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. (MLM 15% ë“±)

TPU `v3-8` ì„ ì´ìš©í•´ ê°ê° 3ì¼, Nì¼(LargeëŠ” í•™ìŠµ ì§„í–‰ ì¤‘)ì„ ì§„í–‰í–ˆê³ , í˜„ì¬ Huggingfaceì— ê³µê°œëœ ëª¨ë¸ì€ 1m(100ë§Œ) stepì„ í•™ìŠµí•œ ckptê°€ ì—…ë¡œë“œ ë˜ì–´ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ í•™ìŠµ LossëŠ” Stepì— ë”°ë¼ ì´ˆê¸° 200kì— ê°€ì¥ ë¹ ë¥´ê²Œ Lossê°€ ì¤„ì–´ë“¤ë‹¤ 400kì´í›„ë¡œëŠ” ì¡°ê¸ˆì”© ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![KcBERT-Base Pretraining Loss](https://d1sr4ybm5bj1wl.cloudfront.net/img/typora/image-20200719183852243.38b124.png)

í•™ìŠµì€ GCPì˜ TPU v3-8ì„ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í–ˆê³ , í•™ìŠµ ì‹œê°„ì€ Small ê¸°ì¤€ 3ì¼ì •ë„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. (Large Modelì€ ì•„ì§ í•™ìŠµì¤‘ì…ë‹ˆë‹¤.)

## Example

### HuggingFace MASK LM

[HuggingFace kcbert-base ëª¨ë¸]([https://huggingface.co/beomi/kcbert-base?text=%EC%98%A4%EB%8A%98%EC%9D%80+%EB%82%A0%EC%94%A8%EA%B0%80+%5BMASK%5D](https://huggingface.co/beomi/kcbert-base?text=ì˜¤ëŠ˜ì€+ë‚ ì”¨ê°€+[MASK])) ì—ì„œ ì•„ë˜ì™€ ê°™ì´ í…ŒìŠ¤íŠ¸ í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ "ì¢‹ë„¤ìš”"](https://d1sr4ybm5bj1wl.cloudfront.net/img/typora/image-20200719205919389.5670d6.png)

### NSMC Binary Classification (Acc: `.89048`)

[ë„¤ì´ë²„ ì˜í™”í‰ ì½”í¼ìŠ¤](https://github.com/e9t/nsmc) ë°ì´í„°ì…‹ì„ ëŒ€ìƒìœ¼ë¡œ Fine Tuningì„ ì§„í–‰í•´ ì„±ëŠ¥ì„ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸í•´ë³´ì•˜ìŠµë‹ˆë‹¤.

í•´ë‹¹ í…ŒìŠ¤íŠ¸ ì½”ë“œëŠ” [Colab](https://colab.research.google.com/gist/Beomi/c26cf67f9fb717d81141c579635816b2/kcbert-440k-nsmc.ipynb)ì—ì„œ ì§ì ‘ ì‹¤í–‰í•´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![image-20200719201102895](https://d1sr4ybm5bj1wl.cloudfront.net/img/typora/image-20200719201102895.ddbdfc.png)

> ë” ë‹¤ì–‘í•œ Downstream Taskì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ê³  ê³µê°œí•  ì˜ˆì •ì…ë‹ˆë‹¤.

## Acknowledgement



## Acknowledgement

KcBERT Modelì„ í•™ìŠµí•˜ëŠ” GCP/TPU í™˜ê²½ì€ [TFRC](https://www.tensorflow.org/tfrc?hl=ko) í”„ë¡œê·¸ë¨ì˜ ì§€ì›ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.

ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ë§ì€ ì¡°ì–¸ì„ ì£¼ì‹  [Monologg](https://github.com/monologg/) ë‹˜ ê°ì‚¬í•©ë‹ˆë‹¤ :)

## Reference

### Github Repos

- [BERT by Google](https://github.com/google-research/bert)
- [KoBERT by SKT](https://github.com/SKTBrain/KoBERT)
- [KoELECTRA by Monologg](https://github.com/monologg/KoELECTRA/)

- [Transformers by Huggingface](https://github.com/huggingface/transformers)
- [Tokenizers by Hugginface](https://github.com/huggingface/tokenizers)

### Papers

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

### Blogs

- [Monologgë‹˜ì˜ KoELECTRA í•™ìŠµê¸°](https://monologg.kr/categories/NLP/ELECTRA/)
- [Colabì—ì„œ TPUë¡œ BERT ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¤ê¸° - Tensorflow/Google ver.](/2020/02/26/Train-BERT-from-scratch-on-colab-TPU-Tensorflow-ver/)

